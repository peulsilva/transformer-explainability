{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "while 'notebooks' in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "from src.attention_rollout import AttentionRollout\n",
    "from src.gradient_rollout import AttentionGradRollout\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wordcloud\n",
    "from src.text_utils import remove_stopwords, remove_punctuation, get_word_frequencies, lemmatize_text\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "plt.style.use(['science', 'no-latex'])\n",
    "from src.perturbation import perturb_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AG News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"SetFit/ag_news\", cache_dir='/Data', split = 'train')\\\n",
    "    .train_test_split(test_size=1000, train_size=2000)\n",
    "\n",
    "test_dataset = load_dataset(\"SetFit/ag_news\", cache_dir='/Data', split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.bincount(train_dataset['train']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {\n",
    "    0 : \"World\",\n",
    "    1 : \"Sports\",\n",
    "    2 : \"Business\",\n",
    "    3 : \"Sci/Tech\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"peulsilva/bert-ag_news\", \n",
    "    attn_implementation = 'eager', \n",
    "    num_labels = n_classes,\n",
    "    cache_dir = '/Data'\n",
    ")\\\n",
    "    .to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_dataset['train'], batch_size=batch_size)\n",
    "val_dataloader = DataLoader(train_dataset['test'], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = AttentionRollout(model, attention_layer_name='attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:17<00:00, 56.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.0 best tokens. Accuracy = 0.8849999904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:18<00:00, 53.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.1 best tokens. Accuracy = 0.8709999918937683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:19<00:00, 52.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.2 best tokens. Accuracy = 0.8270000219345093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:18<00:00, 52.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.30000000000000004 best tokens. Accuracy = 0.7649999856948853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:19<00:00, 52.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.4 best tokens. Accuracy = 0.6980000138282776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:18<00:00, 52.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.5 best tokens. Accuracy = 0.6370000243186951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:18<00:00, 53.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.6000000000000001 best tokens. Accuracy = 0.5709999799728394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:18<00:00, 53.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.7000000000000001 best tokens. Accuracy = 0.49799999594688416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:19<00:00, 52.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.8 best tokens. Accuracy = 0.42500001192092896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:19<00:00, 52.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.9 best tokens. Accuracy = 0.3499999940395355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accs = {}\n",
    "\n",
    "ks = np.arange(0, 1., 0.1)\n",
    "\n",
    "for k in ks:\n",
    "    y_pred_test = torch.Tensor([])\n",
    "    y_true_test = torch.Tensor([])\n",
    "    for row in tqdm(val_dataloader):\n",
    "        tokens = tokenizer(\n",
    "            row['text'], \n",
    "            return_tensors='pt',\n",
    "            padding = 'longest'\n",
    "        )\n",
    "\n",
    "        out, attn_matrix = metric(**tokens.to(device), output_attentions = True)\n",
    "\n",
    "        new_input = perturb_text(tokens, attn_matrix.squeeze(), k, tokenizer)\n",
    "        # print(new_input['input_ids'].shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(**new_input.to(device))\n",
    "\n",
    "        y_pred = out.logits.argmax().to('cpu').unsqueeze(dim= 0 )\n",
    "        y_true = row['label'].to('cpu')\n",
    "\n",
    "\n",
    "        y_pred_test = torch.concat([y_pred_test, y_pred])\n",
    "        y_true_test = torch.concat([y_true_test, y_true])\n",
    "\n",
    "        acc = (y_true_test == y_pred_test).sum()/len(val_dataloader)\n",
    "\n",
    "        accs[k] = acc\n",
    "\n",
    "    print(f\"Removed {k} best tokens. Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(accs).apply(lambda x: x.item())\\\n",
    "    .to_pickle(\"data/results/attn_rollout_perturbation.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient attention rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = AttentionGradRollout(model, attention_layer_name='attention.self.dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(val_dataloader):\n\u001b[1;32m      9\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     10\u001b[0m         row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     11\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m         padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m     out, attn_matrix \u001b[38;5;241m=\u001b[39m metric(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokens\u001b[38;5;241m.\u001b[39mto(device), output_attentions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m     new_input \u001b[38;5;241m=\u001b[39m perturb_text(tokens, attn_matrix\u001b[38;5;241m.\u001b[39msqueeze(), k, tokenizer)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# print(new_input['input_ids'].shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/transformer-explainability/src/gradient_rollout.py:74\u001b[0m, in \u001b[0;36mAttentionGradRollout.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattentions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_gradients \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 74\u001b[0m category_index \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     76\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "\n",
    "accs = {}\n",
    "\n",
    "ks = np.arange(0, 1., 0.1)\n",
    "\n",
    "for k in ks:\n",
    "    y_pred_test = torch.Tensor([])\n",
    "    y_true_test = torch.Tensor([])\n",
    "    for row in tqdm(val_dataloader):\n",
    "        tokens = tokenizer(\n",
    "            row['text'], \n",
    "            return_tensors='pt',\n",
    "            padding = 'longest'\n",
    "        )\n",
    "\n",
    "        tokens['labels'] = row['label']\n",
    "\n",
    "        out, attn_matrix = metric(**tokens.to(device), output_attentions = True)\n",
    "\n",
    "        new_input = perturb_text(tokens, attn_matrix.squeeze(), k, tokenizer)\n",
    "        # print(new_input['input_ids'].shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(**new_input.to(device))\n",
    "\n",
    "        y_pred = out.logits.argmax().to('cpu').unsqueeze(dim= 0 )\n",
    "        y_true = row['label'].to('cpu')\n",
    "\n",
    "\n",
    "        y_pred_test = torch.concat([y_pred_test, y_pred])\n",
    "        y_true_test = torch.concat([y_true_test, y_true])\n",
    "\n",
    "        acc = (y_true_test == y_pred_test).sum()/len(val_dataloader)\n",
    "\n",
    "        accs[k] = acc\n",
    "\n",
    "    print(f\"Removed {k} best tokens. Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
